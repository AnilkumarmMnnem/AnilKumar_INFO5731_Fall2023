{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnilkumarmMnnem/AnilKumar_INFO5731_Fall2023/blob/main/Mannem_AnilKumar_Excercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr23Pq4tZMAJ",
        "outputId": "1a422e33-cffa-42a8-c075-94ff58adc038"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.34.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEa_HYynOsXv",
        "outputId": "d2f1d882-102b-4ca9-bfae-a24ca85fdff9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1a5d93fd-2bee-4993-843b-ea4028f596f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nText Classification:\\n\\nit is task of classifiying data into different categories based on the given context, all the categories must be predefiend by which they would be classified.\\n\\nConsidering customer reviews as the data that needs to be categorized, we can categorirze them into positive reviews, negative reviews, neutral reviews.\\n\\nSome of the features for the text classification model would be:\\n\\n1.Bag-of-Words: BoW breaks down a customer review into a set of words, disregarding their order but considering their frequency. This helps the model understand which words are commonly associated with positive or negative sentiments in customer reviews.\\n2.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF emphasizes words that are important in a specific review but not common across all reviews. It helps the model focus on words that are unique to a review and may carry sentiment information.\\n3.Word Embeddings: Word embeddings represent words as vectors in a semantic space. In customer reviews, this helps the model understand the contextual meaning of words, capturing the relationships between words associated with positive or negative sentiments.\\n4.Sentiment Lexicons: Sentiment lexicons provide a pre-defined sentiment score for words. In customer reviews, they help the model assign sentiment scores based on words commonly associated with positive or negative sentiments.\\n5.Negation Handling:  Negation handling is crucial for customer reviews. Detecting words like \"not\" or other negation words helps the model understand when sentiments are negated, distinguishing between \"good\" and \"not good.\"\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Text Classification:\n",
        "\n",
        "it is task of classifiying data into different categories based on the given context, all the categories must be predefiend by which they would be classified.\n",
        "\n",
        "Considering customer reviews as the data that needs to be categorized, we can categorirze them into positive reviews, negative reviews, neutral reviews.\n",
        "\n",
        "Some of the features for the text classification model would be:\n",
        "\n",
        "1.Bag-of-Words: BoW breaks down a customer review into a set of words, disregarding their order but considering their frequency. This helps the model understand which words are commonly associated with positive or negative sentiments in customer reviews.\n",
        "2.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF emphasizes words that are important in a specific review but not common across all reviews. It helps the model focus on words that are unique to a review and may carry sentiment information.\n",
        "3.Word Embeddings: Word embeddings represent words as vectors in a semantic space. In customer reviews, this helps the model understand the contextual meaning of words, capturing the relationships between words associated with positive or negative sentiments.\n",
        "4.Sentiment Lexicons: Sentiment lexicons provide a pre-defined sentiment score for words. In customer reviews, they help the model assign sentiment scores based on words commonly associated with positive or negative sentiments.\n",
        "5.Negation Handling:  Negation handling is crucial for customer reviews. Detecting words like \"not\" or other negation words helps the model understand when sentiments are negated, distinguishing between \"good\" and \"not good.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c133b5-0397-460a-9ab7-712e90b1f4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Embeddings:\n",
            "[[-3.3692860e-03  5.2843144e-04  1.4779960e-03 ... -3.0419662e-05\n",
            "   4.1776561e-04  1.6272439e-03]\n",
            " [ 4.1583809e-04 -1.0212656e-04  3.0056669e-03 ... -6.3653913e-04\n",
            "   1.6618328e-03 -1.2345325e-03]\n",
            " [-2.9480902e-03  1.6748591e-03  1.1765980e-03 ... -1.2962651e-03\n",
            "   2.8062589e-04 -5.1784283e-04]\n",
            " ...\n",
            " [ 5.1345150e-06  3.0291951e-04  6.3424889e-04 ... -2.5208862e-03\n",
            "  -1.9284795e-05 -7.1709178e-04]\n",
            " [-1.6254347e-03  1.2684626e-03 -4.0522933e-05 ... -1.1427930e-03\n",
            "   1.4445231e-03 -1.2007696e-03]\n",
            " [-6.5461674e-04  1.0356701e-03  1.1300414e-03 ... -4.7053890e-03\n",
            "   1.6661817e-05 -5.7411555e-04]]\n",
            "\n",
            "Sentiment Lexicons:\n",
            "[{'neg': 0.0, 'neu': 0.464, 'pos': 0.536, 'compound': 0.8439}, {'neg': 0.372, 'neu': 0.628, 'pos': 0.0, 'compound': -0.5773}, {'neg': 0.0, 'neu': 0.492, 'pos': 0.508, 'compound': 0.7339}, {'neg': 0.307, 'neu': 0.693, 'pos': 0.0, 'compound': -0.4767}, {'neg': 0.238, 'neu': 0.429, 'pos': 0.334, 'compound': 0.2769}, {'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.8439}, {'neg': 0.593, 'neu': 0.407, 'pos': 0.0, 'compound': -0.8807}, {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.3089}, {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6124}, {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.4767}, {'neg': 0.0, 'neu': 0.413, 'pos': 0.587, 'compound': 0.9299}, {'neg': 0.134, 'neu': 0.698, 'pos': 0.169, 'compound': 0.0772}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.398, 'neu': 0.602, 'pos': 0.0, 'compound': -0.6486}, {'neg': 0.139, 'neu': 0.667, 'pos': 0.194, 'compound': 0.1695}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.256, 'neu': 0.744, 'pos': 0.0, 'compound': -0.4767}, {'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.2716}, {'neg': 0.3, 'neu': 0.7, 'pos': 0.0, 'compound': -0.4588}, {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.4767}, {'neg': 0.411, 'neu': 0.589, 'pos': 0.0, 'compound': -0.7425}, {'neg': 0.197, 'neu': 0.637, 'pos': 0.166, 'compound': -0.128}, {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.457, 'pos': 0.543, 'compound': 0.8591}]\n",
            "\n",
            "Bag-of-Words:\n",
            "['about' 'above' 'again' 'all' 'amazing' 'and' 'as' 'assist' 'at'\n",
            " 'average' 'avoid' 'awful' 'be' 'beautiful' 'best' 'beyond' 'but' 'buy'\n",
            " 'buying' 'company' 'contacted' 'costs' 'couldn' 'customer' 'damaged'\n",
            " 'delivery' 'didn' 'disappointed' 'do' 'easy' 'ever' 'exceeded'\n",
            " 'excellent' 'exceptional' 'expectations' 'expected' 'experience'\n",
            " 'extraordinary' 'family' 'fantastic' 'fast' 'for' 'forever' 'found'\n",
            " 'friends' 'from' 'good' 'great' 'had' 'happier' 'high' 'highly'\n",
            " 'impressed' 'is' 'issue' 'it' 'item' 'looking' 'love' 'low' 'made' 'me'\n",
            " 'meet' 'meh' 'my' 'navigate' 'not' 'notch' 'nothing' 'okay' 'or'\n",
            " 'outstanding' 'overpriced' 'packaging' 'poor' 'price' 'product'\n",
            " 'products' 'prompt' 'purchase' 'quality' 'quick' 'reasonable' 'received'\n",
            " 'recommend' 'recommended' 'refund' 'regret' 'repeat' 'resolve' 'rude'\n",
            " 'satisfied' 'scratches' 'service' 'shipping' 'shoddy' 'special'\n",
            " 'terrible' 'the' 'they' 'this' 'time' 'to' 'took' 'top' 'unhelpful'\n",
            " 'unreliable' 've' 'very' 'was' 'website' 'went' 'will' 'with' 'work'\n",
            " 'worst' 'worth' 'wrong']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "\n",
            "TF-IDF:\n",
            "['about' 'above' 'again' 'all' 'amazing' 'and' 'as' 'assist' 'at'\n",
            " 'average' 'avoid' 'awful' 'be' 'beautiful' 'best' 'beyond' 'but' 'buy'\n",
            " 'buying' 'company' 'contacted' 'costs' 'couldn' 'customer' 'damaged'\n",
            " 'delivery' 'didn' 'disappointed' 'do' 'easy' 'ever' 'exceeded'\n",
            " 'excellent' 'exceptional' 'expectations' 'expected' 'experience'\n",
            " 'extraordinary' 'family' 'fantastic' 'fast' 'for' 'forever' 'found'\n",
            " 'friends' 'from' 'good' 'great' 'had' 'happier' 'high' 'highly'\n",
            " 'impressed' 'is' 'issue' 'it' 'item' 'looking' 'love' 'low' 'made' 'me'\n",
            " 'meet' 'meh' 'my' 'navigate' 'not' 'notch' 'nothing' 'okay' 'or'\n",
            " 'outstanding' 'overpriced' 'packaging' 'poor' 'price' 'product'\n",
            " 'products' 'prompt' 'purchase' 'quality' 'quick' 'reasonable' 'received'\n",
            " 'recommend' 'recommended' 'refund' 'regret' 'repeat' 'resolve' 'rude'\n",
            " 'satisfied' 'scratches' 'service' 'shipping' 'shoddy' 'special'\n",
            " 'terrible' 'the' 'they' 'this' 'time' 'to' 'took' 'top' 'unhelpful'\n",
            " 'unreliable' 've' 'very' 'was' 'website' 'went' 'will' 'with' 'work'\n",
            " 'worst' 'worth' 'wrong']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "Punctuation and Emoji Analysis:\n",
            "[[1, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
            "\n",
            "Sentence Structure:\n",
            "[[1, 11], [2, 12], [2, 10], [2, 10], [2, 10], [2, 10], [2, 12], [2, 11], [1, 7], [2, 14], [3, 16], [2, 11], [2, 13], [2, 12], [2, 12], [2, 10], [2, 13], [2, 9], [2, 10], [2, 14], [2, 14], [2, 14], [2, 17], [2, 15], [2, 13]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample customer reviews\n",
        "reviews = [\n",
        "    \"The product is excellent, and the service is great!\",\n",
        "    \"Not satisfied with the quality. The product is not good.\",\n",
        "    \"Amazing experience with the customer service. Highly recommended!\",\n",
        "    \"Disappointed with the delivery time. It took forever.\",\n",
        "    \"This product is fantastic. Couldn't be happier!\",\n",
        "    \"Great product! Fast shipping and excellent customer service.\",\n",
        "    \"The worst purchase ever. Terrible quality and rude customer service.\",\n",
        "    \"Average experience. Nothing special about the product or service.\",\n",
        "    \"Outstanding service and top-notch product quality.\",\n",
        "    \"Do not buy from this company. Poor customer service and low-quality products.\",\n",
        "    \"Love it! The best purchase I've made. Quick delivery and great quality.\",\n",
        "    \"Meh. The product was okay, but nothing extraordinary.\",\n",
        "    \"Exceptional customer service! They went above and beyond to assist me.\",\n",
        "    \"Avoid at all costs. Terrible customer service and unreliable products.\",\n",
        "    \"Overpriced and not worth it. Regret buying from this company.\",\n",
        "    \"Prompt delivery and high-quality product. Will buy again!\",\n",
        "    \"The product didn't meet my expectations. Disappointed with the purchase.\",\n",
        "    \"Highly recommended. The product exceeded my expectations.\",\n",
        "    \"Awful experience. Shoddy product and unhelpful customer service.\",\n",
        "    \"Impressed with the quality and quick shipping. Will be a repeat customer.\",\n",
        "    \"The packaging was damaged, and the product had scratches. Very disappointed.\",\n",
        "    \"Received the wrong item. Customer service was quick to resolve the issue.\",\n",
        "    \"Easy to navigate website. Found the product I was looking for at a reasonable price.\",\n",
        "    \"The product didn't work as expected. Contacted customer service for a refund.\",\n",
        "    \"Beautiful packaging and high-quality product. Will recommend to friends and family.\",\n",
        "]\n",
        "\n",
        "# Word Embeddings (using Word2Vec)\n",
        "word_tokens = [word_tokenize(review.lower()) for review in reviews]\n",
        "model_w2v = Word2Vec(sentences=word_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "embeddings = [np.mean([model_w2v.wv[word] for word in tokens], axis=0) for tokens in word_tokens]\n",
        "print(\"\\nWord Embeddings:\")\n",
        "print(np.array(embeddings))\n",
        "\n",
        "# Sentiment Lexicons (using NLTK SentimentIntensityAnalyzer)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = [sia.polarity_scores(review) for review in reviews]\n",
        "print(\"\\nSentiment Lexicons:\")\n",
        "print(sentiment_scores)\n",
        "\n",
        "# Bag-of-Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(reviews)\n",
        "print(\"\\nBag-of-Words:\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(reviews)\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n",
        "\n",
        "# Punctuation and Emoji Analysis\n",
        "punctuation_features = [[review.count('!'), review.count('?'), review.count('😊'), review.count('😞')] for review in reviews]\n",
        "print(\"\\nPunctuation and Emoji Analysis:\")\n",
        "print(punctuation_features)\n",
        "\n",
        "# Sentence Structure (using NLTK)\n",
        "sentence_lengths = [[len(nltk.sent_tokenize(review)), len(word_tokenize(review))] for review in reviews]\n",
        "print(\"\\nSentence Structure:\")\n",
        "print(sentence_lengths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b893b2-4f78-42c6-b184-37e94823c1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features:\n",
            "resolveresolvesentiment_score: 0.2016691334644629\n",
            "wrongwrongsentiment_score: 0.132051528846858\n",
            "regretregretsentiment_score: 0.1314324812278107\n",
            "worthworthsentiment_score: 0.1164324812278108\n",
            "fastfastsentiment_score: 0.11405152884685843\n",
            "productsproductssentiment_score: 0.11352771932304861\n",
            "mymysentiment_score: 0.10888991268524184\n",
            "contactedcontactedsentiment_score: 0.10739929219462163\n",
            "buyingbuyingsentiment_score: 0.10552771932304861\n",
            "toptopsentiment_score: 0.1039707207660503\n",
            "didndidnsentiment_score: 0.10394041773574703\n",
            "veryverysentiment_score: 0.09921025900558855\n",
            "overpricedoverpricedsentiment_score: 0.09877880157413088\n",
            "bestbestsentiment_score: 0.0940039097992389\n",
            "isissentiment_score: 0.09017562697095638\n",
            "vevesentiment_score: 0.08351484331017267\n",
            "amazingamazingsentiment_score: 0.07252771932304869\n",
            "outstandingoutstandingsentiment_score: 0.06854314833847774\n",
            "dodosentiment_score: 0.06682930662463593\n",
            "nothingnothingsentiment_score: 0.06459121138654056\n",
            "greatgreatsentiment_score: 0.0645272753226045\n",
            "unhelpfulunhelpfulsentiment_score: 0.06381343360876279\n",
            "repeatrepeatsentiment_score: 0.061829306624635594\n",
            "worstworstsentiment_score: 0.061067401862731074\n",
            "thisthissentiment_score: 0.056829306624636144\n",
            "familyfamilysentiment_score: 0.053940417735747204\n",
            "avoidavoidsentiment_score: 0.0519404177357472\n",
            "receivedreceivedsentiment_score: 0.05072019351552304\n",
            "itemitemsentiment_score: 0.0456070844024139\n",
            "notchnotchsentiment_score: 0.04430549710082654\n",
            "promptpromptsentiment_score: 0.04327375106908038\n",
            "beautifulbeautifulsentiment_score: 0.03843248122781073\n",
            "notnotsentiment_score: 0.03333724313257269\n",
            "expectationsexpectationssentiment_score: 0.029226132021461293\n",
            "withwithsentiment_score: 0.028051528846858353\n",
            "shippingshippingsentiment_score: 0.026640828436158115\n",
            "foreverforeversentiment_score: 0.025747055542384878\n",
            "purchasepurchasesentiment_score: 0.01381343360876297\n",
            "websitewebsitesentiment_score: 0.011908671704001339\n",
            "companycompanysentiment_score: 0.009305497100826399\n",
            "mehmehsentiment_score: 0.006575338370667927\n",
            "aboutaboutsentiment_score: 0.0\n",
            "aboveabovesentiment_score: 0.0\n",
            "againagainsentiment_score: 0.0\n",
            "allallsentiment_score: 0.0\n",
            "andandsentiment_score: 0.0\n",
            "asassentiment_score: 0.0\n",
            "assistassistsentiment_score: 0.0\n",
            "atatsentiment_score: 0.0\n",
            "averageaveragesentiment_score: 0.0\n",
            "awfulawfulsentiment_score: 0.0\n",
            "bebesentiment_score: 0.0\n",
            "beyondbeyondsentiment_score: 0.0\n",
            "butbutsentiment_score: 0.0\n",
            "buybuysentiment_score: 0.0\n",
            "costscostssentiment_score: 0.0\n",
            "couldncouldnsentiment_score: 0.0\n",
            "customercustomersentiment_score: 0.0\n",
            "damageddamagedsentiment_score: 0.0\n",
            "deliverydeliverysentiment_score: 0.0\n",
            "disappointeddisappointedsentiment_score: 0.0\n",
            "easyeasysentiment_score: 0.0\n",
            "evereversentiment_score: 0.0\n",
            "exceededexceededsentiment_score: 0.0\n",
            "excellentexcellentsentiment_score: 0.0\n",
            "exceptionalexceptionalsentiment_score: 0.0\n",
            "expectedexpectedsentiment_score: 0.0\n",
            "experienceexperiencesentiment_score: 0.0\n",
            "extraordinaryextraordinarysentiment_score: 0.0\n",
            "fantasticfantasticsentiment_score: 0.0\n",
            "forforsentiment_score: 0.0\n",
            "foundfoundsentiment_score: 0.0\n",
            "friendsfriendssentiment_score: 0.0\n",
            "fromfromsentiment_score: 0.0\n",
            "goodgoodsentiment_score: 0.0\n",
            "hadhadsentiment_score: 0.0\n",
            "happierhappiersentiment_score: 0.0\n",
            "highhighsentiment_score: 0.0\n",
            "highlyhighlysentiment_score: 0.0\n",
            "impressedimpressedsentiment_score: 0.0\n",
            "issueissuesentiment_score: 0.0\n",
            "ititsentiment_score: 0.0\n",
            "lookinglookingsentiment_score: 0.0\n",
            "lovelovesentiment_score: 0.0\n",
            "lowlowsentiment_score: 0.0\n",
            "mademadesentiment_score: 0.0\n",
            "memesentiment_score: 0.0\n",
            "meetmeetsentiment_score: 0.0\n",
            "navigatenavigatesentiment_score: 0.0\n",
            "okayokaysentiment_score: 0.0\n",
            "ororsentiment_score: 0.0\n",
            "packagingpackagingsentiment_score: 0.0\n",
            "poorpoorsentiment_score: 0.0\n",
            "pricepricesentiment_score: 0.0\n",
            "productproductsentiment_score: 0.0\n",
            "qualityqualitysentiment_score: 0.0\n",
            "quickquicksentiment_score: 0.0\n",
            "reasonablereasonablesentiment_score: 0.0\n",
            "recommendrecommendsentiment_score: 0.0\n",
            "recommendedrecommendedsentiment_score: 0.0\n",
            "refundrefundsentiment_score: 0.0\n",
            "ruderudesentiment_score: 0.0\n",
            "satisfiedsatisfiedsentiment_score: 0.0\n",
            "scratchesscratchessentiment_score: 0.0\n",
            "serviceservicesentiment_score: 0.0\n",
            "shoddyshoddysentiment_score: 0.0\n",
            "specialspecialsentiment_score: 0.0\n",
            "terribleterriblesentiment_score: 0.0\n",
            "thethesentiment_score: 0.0\n",
            "theytheysentiment_score: 0.0\n",
            "timetimesentiment_score: 0.0\n",
            "totosentiment_score: 0.0\n",
            "tooktooksentiment_score: 0.0\n",
            "unreliableunreliablesentiment_score: 0.0\n",
            "waswassentiment_score: 0.0\n",
            "wentwentsentiment_score: 0.0\n",
            "willwillsentiment_score: 0.0\n",
            "workworksentiment_score: 0.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import numpy as np\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sample customer reviews and corresponding labels (positive=1, negative=0)\n",
        "reviews = [\n",
        "    \"The product is excellent, and the service is great!\",\n",
        "    \"Not satisfied with the quality. The product is not good.\",\n",
        "    \"Amazing experience with the customer service. Highly recommended!\",\n",
        "    \"Disappointed with the delivery time. It took forever.\",\n",
        "    \"This product is fantastic. Couldn't be happier!\",\n",
        "    \"Great product! Fast shipping and excellent customer service.\",\n",
        "    \"The worst purchase ever. Terrible quality and rude customer service.\",\n",
        "    \"Average experience. Nothing special about the product or service.\",\n",
        "    \"Outstanding service and top-notch product quality.\",\n",
        "    \"Do not buy from this company. Poor customer service and low-quality products.\",\n",
        "    \"Love it! The best purchase I've made. Quick delivery and great quality.\",\n",
        "    \"Meh. The product was okay, but nothing extraordinary.\",\n",
        "    \"Exceptional customer service! They went above and beyond to assist me.\",\n",
        "    \"Avoid at all costs. Terrible customer service and unreliable products.\",\n",
        "    \"Overpriced and not worth it. Regret buying from this company.\",\n",
        "    \"Prompt delivery and high-quality product. Will buy again!\",\n",
        "    \"The product didn't meet my expectations. Disappointed with the purchase.\",\n",
        "    \"Highly recommended. The product exceeded my expectations.\",\n",
        "    \"Awful experience. Shoddy product and unhelpful customer service.\",\n",
        "    \"Impressed with the quality and quick shipping. Will be a repeat customer.\",\n",
        "    \"The packaging was damaged, and the product had scratches. Very disappointed.\",\n",
        "    \"Received the wrong item. Customer service was quick to resolve the issue.\",\n",
        "    \"Easy to navigate website. Found the product I was looking for at a reasonable price.\",\n",
        "    \"The product didn't work as expected. Contacted customer service for a refund.\",\n",
        "    \"Beautiful packaging and high-quality product. Will recommend to friends and family.\",\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0]  # Ensure this list has the same length as the number of reviews\n",
        "\n",
        "\n",
        "# Bag-of-Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(reviews).toarray()\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(reviews).toarray()\n",
        "\n",
        "# Sentiment Lexicons (using NLTK SentimentIntensityAnalyzer)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = np.array([sia.polarity_scores(review)[\"compound\"] for review in reviews])\n",
        "\n",
        "# Combine all features into one matrix\n",
        "all_features = np.hstack((X_bow, X_tfidf, sentiment_scores.reshape(-1, 1)))\n",
        "\n",
        "# Feature selection using mutual information\n",
        "mi_scores = mutual_info_classif(all_features, labels, discrete_features=False)\n",
        "feature_names = (\n",
        "    vectorizer_bow.get_feature_names_out() +\n",
        "    vectorizer_tfidf.get_feature_names_out() +\n",
        "    [\"sentiment_score\"]\n",
        ")\n",
        "\n",
        "# Create a dictionary of feature names and their corresponding importance scores\n",
        "feature_importance_dict = dict(zip(feature_names, mi_scores))\n",
        "\n",
        "# Rank features based on importance in descending order\n",
        "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the ranked features\n",
        "print(\"Ranked Features:\")\n",
        "for feature, importance in sorted_features:\n",
        "    print(f\"{feature}: {importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e76c5a1-2c4e-4b25-ec20-54b9467acc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity:\n",
            "Document 9: Similarity Score = 0.8452\n",
            "Document 6: Similarity Score = 0.7981\n",
            "Document 3: Similarity Score = 0.7297\n",
            "Document 1: Similarity Score = 0.7253\n",
            "Document 16: Similarity Score = 0.6955\n",
            "Document 25: Similarity Score = 0.6706\n",
            "Document 20: Similarity Score = 0.6622\n",
            "Document 11: Similarity Score = 0.6517\n",
            "Document 13: Similarity Score = 0.6129\n",
            "Document 10: Similarity Score = 0.6045\n",
            "Document 5: Similarity Score = 0.5547\n",
            "Document 19: Similarity Score = 0.5273\n",
            "Document 14: Similarity Score = 0.5118\n",
            "Document 2: Similarity Score = 0.4839\n",
            "Document 18: Similarity Score = 0.4662\n",
            "Document 23: Similarity Score = 0.4398\n",
            "Document 8: Similarity Score = 0.4154\n",
            "Document 7: Similarity Score = 0.3969\n",
            "Document 15: Similarity Score = 0.3682\n",
            "Document 24: Similarity Score = 0.3670\n",
            "Document 22: Similarity Score = 0.3612\n",
            "Document 12: Similarity Score = 0.3514\n",
            "Document 17: Similarity Score = 0.2200\n",
            "Document 21: Similarity Score = 0.1469\n",
            "Document 4: Similarity Score = 0.0222\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Sample customer reviews\n",
        "reviews = [\n",
        "    \"The product is excellent, and the service is great!\",\n",
        "    \"Not satisfied with the quality. The product is not good.\",\n",
        "    \"Amazing experience with the customer service. Highly recommended!\",\n",
        "    \"Disappointed with the delivery time. It took forever.\",\n",
        "    \"This product is fantastic. Couldn't be happier!\",\n",
        "    \"Great product! Fast shipping and excellent customer service.\",\n",
        "    \"The worst purchase ever. Terrible quality and rude customer service.\",\n",
        "    \"Average experience. Nothing special about the product or service.\",\n",
        "    \"Outstanding service and top-notch product quality.\",\n",
        "    \"Do not buy from this company. Poor customer service and low-quality products.\",\n",
        "    \"Love it! The best purchase I've made. Quick delivery and great quality.\",\n",
        "    \"Meh. The product was okay, but nothing extraordinary.\",\n",
        "    \"Exceptional customer service! They went above and beyond to assist me.\",\n",
        "    \"Avoid at all costs. Terrible customer service and unreliable products.\",\n",
        "    \"Overpriced and not worth it. Regret buying from this company.\",\n",
        "    \"Prompt delivery and high-quality product. Will buy again!\",\n",
        "    \"The product didn't meet my expectations. Disappointed with the purchase.\",\n",
        "    \"Highly recommended. The product exceeded my expectations.\",\n",
        "    \"Awful experience. Shoddy product and unhelpful customer service.\",\n",
        "    \"Impressed with the quality and quick shipping. Will be a repeat customer.\",\n",
        "    \"The packaging was damaged, and the product had scratches. Very disappointed.\",\n",
        "    \"Received the wrong item. Customer service was quick to resolve the issue.\",\n",
        "    \"Easy to navigate website. Found the product I was looking for at a reasonable price.\",\n",
        "    \"The product didn't work as expected. Contacted customer service for a refund.\",\n",
        "    \"Beautiful packaging and high-quality product. Will recommend to friends and family.\",\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"Great customer service and high-quality products\"\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the query and reviews\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "review_embeddings = model.encode(reviews, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, review_embeddings)[0]\n",
        "\n",
        "# Create a list of (index, cosine similarity) pairs\n",
        "similarity_scores = [(i, cosine_scores[i].item()) for i in range(len(cosine_scores))]\n",
        "\n",
        "# Rank documents based on similarity in descending order\n",
        "ranked_documents = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the ranked documents\n",
        "print(\"Ranked Documents based on Similarity:\")\n",
        "for idx, score in ranked_documents:\n",
        "    print(f\"Document {idx + 1}: Similarity Score = {score:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLu93b3qZCOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}